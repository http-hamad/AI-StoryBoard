{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "from dotenv import load_dotenv\n",
    "from PIL import Image\n",
    "import requests\n",
    "import json\n",
    "import google.generativeai as genai\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from PIL import Image\n",
    "from groq import Groq\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "gemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "deepseek_api_key = os.getenv(\"DEEPSEEK_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Image captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the BLIP processor and model\n",
    "processor = BlipProcessor.from_pretrained(\"Sof22/image-caption-large-copy\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Sof22/image-caption-large-copy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate image captions\n",
    "def generate_caption(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")  # Load the image\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")  # Preprocess the image\n",
    "    with torch.no_grad():\n",
    "        caption_ids = model.generate(**inputs)  # Generate caption IDs\n",
    "    caption = processor.decode(caption_ids[0], skip_special_tokens=True)  # Decode to text\n",
    "    return caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with an image\n",
    "image_path = \"img.jpg\" \n",
    "caption = generate_caption(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Caption: araffe standing in front of a church with a sky background and a person holding a cell phone\n"
     ]
    }
   ],
   "source": [
    "# Print the generated caption\n",
    "print(\"Generated Caption:\", caption)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrating LLM for story generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google gemanie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The giraffe stood silently before the weathered church, its long neck bowed.  A bruised purple sky mirrored the sorrow in its large, gentle eyes.  Below, a lone figure, hunched against the chill wind, held a cell phone, a blurry photo displayed on the screen – a younger giraffe, playful and vibrant, beside a smiling child.  The phone slipped from numb fingers; the image, a painful reminder of a life lost, a bond severed, flickered before fading to black, reflecting the emptiness in the giraffe’s soulful gaze, and in the heart of the mourner.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "genai.configure(api_key=gemini_api_key)\n",
    "model = genai.GenerativeModel(\"models/gemini-1.5-flash\")\n",
    "\n",
    "#settings of the story\n",
    "#length\n",
    "n = input(\"Enter the lenght of story to generate: \")\n",
    "#theme\n",
    "theme = input(\"Enter the theme of the story: \")\n",
    "\n",
    "# Generate text\n",
    "prompt = (f\"Write a {n}-word long story about {caption} and give it a touch of {theme} theme. \"\n",
    "          \"Start your response from the beginning of the story and conclude it at the end.\")\n",
    "\n",
    "# n = input(\"Enter the lenght of story to generate: \")\n",
    "result = model.generate_content(prompt)\n",
    "\n",
    "result = result.candidates[0].content.parts[0].text\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deepseek\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The giraffe stood alone in front of the old church, its tall silhouette contrasting against the vast, cloudy sky. A tourist approached, phone in hand, but instead of excitement, their eyes carried sadness. They had come to capture the moment, yet the giraffe’s stillness felt like a mirror to their own loneliness. The phone remained lowered as they thought of someone they once shared such sights with, now gone. The giraffe, unaware, gazed at the heavens, its reflection in a nearby puddle rippling like unspoken tears. The tourist sighed, turned, and walked away, leaving the giraffe to its quiet vigil.\n"
     ]
    }
   ],
   "source": [
    "#settings of the story\n",
    "#length\n",
    "n = input(\"Enter the lenght of story to generate: \")\n",
    "#theme\n",
    "theme = input(\"Enter the theme of the story: \")\n",
    "\n",
    "client = Groq(api_key=deepseek_api_key)\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"deepseek-r1-distill-llama-70b\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": (f\"Write a {n}-word long story about {caption} and give it a touch of {theme} theme. \"\n",
    "                        \"Start your response from the beginning of the story and conclude it at the end.\")\n",
    "        }\n",
    "    ],\n",
    "    temperature=0.6,\n",
    "    max_completion_tokens=1024,\n",
    "    top_p=0.95,\n",
    "    stream=True,\n",
    "    reasoning_format=\"raw\"\n",
    ")\n",
    "\n",
    "story_output = \"\"\n",
    "for chunk in completion:\n",
    "    story_output += chunk.choices[0].delta.content or \"\"\n",
    "\n",
    "# Remove text between <think> and </think>\n",
    "clean_story = re.sub(r\"<think>.*?</think>\", \"\", story_output, flags=re.DOTALL)\n",
    "\n",
    "# Print the final story\n",
    "print(clean_story.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qwen 2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blip_dep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
